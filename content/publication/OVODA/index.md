---
# abstract: The growing capabilities of AI in generating video content have brought forward significant challenges in effectively evaluating these videos. Unlike static images or text, video content involves complex spatial and temporal dynamics which may require a more comprehensive and systematic evaluation of its contents in aspects like video presentation quality, semantic information delivery, alignment with human intentions, and the virtual-reality consistency with our physical world. This survey identifies the emerging field of AI-Generated Video Evaluation (AIGVE), highlighting the importance of assessing how well AI-generated videos align with human perception and meet specific instructions. We provide a structured analysis of existing methodologies that could be potentially used to evaluate AI-generated videos. By outlining the strengths and gaps in current approaches, we advocate for the development of more robust and nuanced evaluation frameworks that can handle the complexities of video content, which include not only the conventional metric-based evaluations, but also the current human-involved evaluations, and the future model-centered evaluations. This survey aims to establish a foundational knowledge base for both researchers from academia and practitioners from the industry, facilitating the future advancement of evaluation methods for AI-generated video content.
abstract: Under review
# slides: example
# url_pdf: "https://downloads.hindawi.com/journals/complexity/2020/8863526.pdf"

publication_types:
  - "Preprint"
authors:
  - Xinhao Xiang
  - Kuan-Chuan Peng
  - Suhas Lohit
  - Michael Jeffrey Jones
  - Jiawei Zhang
# author_notes:
#   - Xiao Liu
#   - Xinhao Xiang
#   - and Zizhong Li contributed equally to this work
# publication: arXiv 
summary: "A novel open-vocabulary multi-modal 3D object detection method which can incorporate multi-view input and detect complex events (including attributes) without needing novel class anchor size; proposed a dataset for open-vocabulary attribute detection"

url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
#url_video: "https://drive.google.com/file/d/1-4rLMaELwzTadFjY1JglA715cdTOvUFk/view?usp=sharing"
title: "Open-Vocabulary Multimodal 3D Object Detection with Attributes"
# doi: https://doi.org/10.1155/2020/8863526
featured: false
# url_pdf: "https://arxiv.org/pdf/2410.19884"

tags: []
# projects:
#   - SUSTech
# image:
#   caption: "Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)"
#   focal_point: ""
#   preview_only: false
date: 2024-11-16T04:39:21.092Z

url_slides: ""
# publishDate: 2020-11-29T04:39:21.092Z
url_poster: ""
# url_code: "https://github.com/ShaneXiangH/E2RPSO"
---

<!-- {{% callout note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

{{% callout note %}}
Create your slides in Markdown - click the *Slides* button to check out the example.
{{% /callout %}} -->

<!-- Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -->
