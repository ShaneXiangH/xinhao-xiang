---
abstract: The rapid advancement in AI-generated video synthesis has led to a growth demand for standardized and effective evaluation metrics. Existing metrics lack a unified framework for systematically categorizing methodologies, limiting a holistic understanding of the evaluation landscape. Additionally, fragmented implementations and the absence of standardized interfaces lead to redundant processing overhead. Furthermore, many prior approaches are constrained by dataset-specific dependencies, limiting their applicability across diverse video domains. To address these challenges, we introduce AIGVE-Tool (AI-Generated Video Evaluation Toolkit), a unified framework that provides a structured and extensible evaluation pipeline for a comprehensive AI-generated video evaluation. Organized within a novel five-category taxonomy, AIGVE-Tool integrates multiple evaluation methodologies while allowing flexible customization through a modular configuration system. Additionally, we propose AIGVE-Bench, a large-scale benchmark dataset created with five SOTA video generation models based on hand-crafted instructions and prompts. This dataset systematically evaluates various video generation models across nine critical quality dimensions. Extensive experiments demonstrate the effectiveness of AIGVE-Tool in providing standardized and reliable evaluation results, highlighting specific strengths and limitations of current models and facilitating the advancements of next-generation AI-generated video techniques.
# slides: example
# url_pdf: "https://downloads.hindawi.com/journals/complexity/2020/8863526.pdf"

# publication_types:
#   - "Preprint"
authors:
  - Xinhao Xiang
  - Xiao Liu
  - Zizhong Li
  - Zhuosheng Liu
  - Jiawei Zhang
author_notes:
  - Xinhao Xiang
  - and Xiao Liu contributed equally to this work
publication_types:
  - "Preprint"
summary: "AIGVE-Tool introduces a unified, modular framework, a benchmark (AIGVE-Bench) for comprehensive, standardized evaluation of AI-generated videos across multiple quality dimensions, and a structured five-category taxonomy of evaluation metrics."

url_dataset: ""
url_project: "https://www.aigve.org/"
publication_short: ""
url_source: "https://github.com/ShaneXiangH/AIGVE_Tool"
#url_video: "https://drive.google.com/file/d/1-4rLMaELwzTadFjY1JglA715cdTOvUFk/view?usp=sharing"
title: "AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted Benchmark"
# doi: https://doi.org/10.1155/2020/8863526
featured: false
url_pdf: "https://arxiv.org/abs/2503.14064"

tags: []
# projects:
#   - SUSTech
# image:
#   caption: "Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)"
#   focal_point: ""
#   preview_only: false
date: 2024-09-16T04:39:21.092Z

url_slides: ""
# publishDate: 2020-11-29T04:39:21.092Z
url_poster: ""
# url_code: "https://github.com/ShaneXiangH/E2RPSO"
---

<!-- {{% callout note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

{{% callout note %}}
Create your slides in Markdown - click the *Slides* button to check out the example.
{{% /callout %}} -->

<!-- Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -->
